{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10815698,"sourceType":"datasetVersion","datasetId":6714880}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CSCA-5642: Final Project #\n#### Develop an temporal sequence classifier to recognize action from video clips. ####\n    \n* Author: Alexander Meau  \n* Email: alme9155@colorado.edu  \n* GitHub: [https://github.com/alme9155/csca-5642-week4/tree/main](https://github.com/alme9155/csca-5642-week4/tree/main)  \n\n<imag src=\"https://storage.googleapis.com/kaggle-datasets-images/2232355/3733921/1aa7cf2963836e7be0a22c2888d60b87/dataset-cover.jpg?t=2022-06-02-06-19-02\" alt=\".\" width=\"300\">","metadata":{}},{"cell_type":"markdown","source":"## I. Brief description of the problem and data ##\n\nThis project explores the application of deep neural networks for human action recognition (HAR) in video sequences. Human action recognition has a wide range of practical applications, including sports analytics, robotics, and surveillance systems. By leveraging architectures such as CNN+LSTM and 3D Convolutional Networks (C3D), this study aims to compare the effectiveness of different algorithmic designs. Key challenges include modeling long-term temporal dependencies and handling variations in video length, lighting conditions, and resolution.\n\n### Dataset: ####\n* The UCF50 dataset is a collection of video clips, such as Baseball pitch, kayaking, diving, etc created by the University of Central Florida, Center for Research in Computer Vision. \n* The UCF50 dataset is a reduced dataset from the original 101 action categories database.\n* Ref: [https://www.crcv.ucf.edu/data/UCF101.php](https://www.crcv.ucf.edu/data/UCF101.php).\n  \n### Data Size and Dimension ####\n* Dataset: Total 6999 avi files acrsso 50 categories.\n* Each category has an average of 100-170 .avi video files (average ~133 per category).\n* Most of the video clips are in the same resolutions (width=320 x height=240 x channel=3)\n* Video lengths varies in temporal resolution (frames per second).\n* Videos includes various viewpoints, scales, camera motion, and lighting conditions, making it a challenging dataset for action recognition tasks. ","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ncategory_counts = {}\ninput_path = '/kaggle/input/ucf50-latest-version/UCF50/'\nfor dirname, _, filenames in os.walk(input_path):\n    avi_file_count = sum(1 for f in filenames if f.endswith('.avi'))\n    \n    if avi_file_count > 0:\n        category_name = os.path.basename(dirname)\n        category_counts[category_name] = avi_file_count\n\ndf_counts = pd.DataFrame(list(category_counts.items()), columns=['Category', '# AVI Files'])\ndf_counts = df_counts.sort_values(by='# AVI Files', ascending=False).reset_index(drop=True)\ndisplay(df_counts)\nprint(f\"\\nTotal .avi file count = {df_counts['# AVI Files'].sum()}\")\nprint(f\"Average .avi file count = {df_counts['# AVI Files'].mean()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T06:14:41.684459Z","iopub.execute_input":"2025-07-28T06:14:41.685064Z","iopub.status.idle":"2025-07-28T06:14:56.544914Z","shell.execute_reply.started":"2025-07-28T06:14:41.685038Z","shell.execute_reply":"2025-07-28T06:14:56.544278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## II. Exploratory Data Analysis (EDA) ##\n- Examine video quality:\n  - Dimension (width x height)\n  - Frame Count and Frame per secound\n  - Total count of video clips\n  - Total count of video category","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport pandas as pd\nfrom collections import defaultdict\n\n\ndataset_root = '/kaggle/input/ucf50-latest-version/UCF50/'\nvideo_profile_stats = defaultdict(lambda: {'Video Count': 0, 'Category Count': set()})\nfor root, _, files in os.walk(dataset_root):\n    for file in files:\n        if file.endswith('.avi'):\n            video_path = os.path.join(root, file)\n            cap = cv2.VideoCapture(video_path)\n\n            if not cap.isOpened():\n                continue\n\n            fps = cap.get(cv2.CAP_PROP_FPS)\n            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            duration = round(frame_count / fps, 1) if fps > 0 else 0\n            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n            resolution = f\"{width}x{height}\"\n            category = os.path.basename(os.path.dirname(video_path))\n            #key = (resolution, round(fps, 2), frame_count, duration)\n            ret, frame = cap.read()\n            channel =1\n            if ret and frame is not None and len(frame.shape) == 3:\n                channel = frame.shape[2]\n            key = (resolution, round(fps, 2), frame_count, duration, channel)\n            \n            # Accumulate count\n            video_profile_stats[key]['Video Count'] += 1\n            video_profile_stats[key]['Category Count'].add(category)\n            cap.release()\n\n# Convert to DataFrame\nsummary_rows = []\nfor key, stats in video_profile_stats.items():\n    resolution, fps, frame_count, duration, channel = key\n    video_count = stats['Video Count']\n    category_count = len(stats['Category Count'])\n    summary_rows.append({\n        'Resolution': resolution,\n        'Frame per sec': fps,\n        'Frame Count': frame_count,\n        'Duration in sec': duration,\n        'Channels': channel,\n        'Video Count': video_count,\n        'Category Count': category_count\n    })\n\nprint(\"\\nAccumulated Video Stats Summary:\\n\")\ndf_summary = pd.DataFrame(summary_rows).sort_values(by='Video Count', ascending=False).reset_index(drop=True)\ndisplay(df_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T06:15:52.996938Z","iopub.execute_input":"2025-07-28T06:15:52.997229Z","iopub.status.idle":"2025-07-28T06:16:52.179007Z","shell.execute_reply.started":"2025-07-28T06:15:52.997206Z","shell.execute_reply":"2025-07-28T06:16:52.178437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncategory_stats = []\nfor key, stats in video_profile_stats.items():\n    resolution, fps, frame_count, duration, channel = key\n    input_shape = f\"{frame_count}x{height}x{width}x3\"  \n    for category in stats['Category Count']:\n        category_stats.append({\n            'Category': category,\n            'Resolution': resolution,\n            'Frame per sec': fps,\n            'Frame Count': frame_count,\n            'Duration in sec': duration,\n            'Channels': channel,\n            'Input Shape': input_shape,\n            'Video Count': stats['Video Count'],  # shared across all category entries here\n        })\n\ndf_category_stats = pd.DataFrame(category_stats)\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=df_category_stats, x='Category', y='Frame Count')\nplt.xticks(rotation=45)\nplt.title(\"Frame Count Distribution by Category\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T06:17:47.351804Z","iopub.execute_input":"2025-07-28T06:17:47.352455Z","iopub.status.idle":"2025-07-28T06:17:48.247950Z","shell.execute_reply.started":"2025-07-28T06:17:47.352433Z","shell.execute_reply":"2025-07-28T06:17:48.247200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.hist(df_counts['# AVI Files'], bins=10, color='skyblue', edgecolor='black')\nplt.title('Distribution of .avi File Counts per Category')\nplt.xlabel('Number of .avi files')\nplt.ylabel('Number of Categories')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T06:17:53.122447Z","iopub.execute_input":"2025-07-28T06:17:53.123064Z","iopub.status.idle":"2025-07-28T06:17:53.326041Z","shell.execute_reply.started":"2025-07-28T06:17:53.123037Z","shell.execute_reply":"2025-07-28T06:17:53.325315Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## III. EDA Summary and Analysis ##\n\n### Summary ###\n* The dataset contains ~7,000 .avi video files (exactly 6,669) across 50 action categories.\n  * Most categories contain between 110 and 150 video clips.\n  * There is no extreme class imbalance in terms of video count per category.\n* Almost all of the videos has same spatial size and color format.\n  * Width x Height x Channel = 320 x 240 x 3\n* There are large variation in frame count.\n  * Frame count ranges from 58 to 641.\n  * Some categories has total frames > 500-800. (e.g. Jump Rope, Horse Race)\n  * Other categories stay under < 150 frames (e.g. Push Ups)\n  * Many action categories contain significant outliers in frame count.\n\n### Analysis ###\n* Due to the volume of dataset, and the limited GPU quotas on Kaggle platform, **10 video categories** have been selected to reduce the input dataset.\n * 10 selected video categories:\n    * **'HighJump', 'Basketball', 'TennisSwing', 'CleanAndJerk', 'PushUps'**\n    * **'JumpingJack', 'SkateBoarding', 'TrampolineJumping', 'Lunges', 'GolfSwing'**\n * Reducing the input dataset can help reduce training time.\n\n* Even though the input dataset is reduced to less number .avi files to process, data preparation is still required to **standardize video input** shape before applying deep learning model.\n* **Spatial Dimension**:\n  *  Deep learning model expect fixed input shapes in the form: (Frames × Height × Width × Channels)\n  *  The OpenCV library returns images in BGR, but most pre-trained models expect RGB format.\n  *  Although the clips are in consistent dimensions (320×240×3), it is best to resize to **112×112×3**\n      *  Reducing the spatial dimensions helps lower memory usage and improves processing speed.\n      *  Pre-trained models such as R(2+1)D and C3D are designed to operate on 112×112x3 inputs.\n* **Temporal Dimension**:\n  * Most pre-trained video classifier required fixed-length input in the range of 16-64 frames.\n  * Uniform sampling to a **32-frame** clip length provides a good balance for our dataset.\n  * For clip <32 frames, clips are padded with the last frame is used to reach the standard length.\n  * For clip >32 frames, clips are split into multiple overlapping chunks of the standard length.\n\n* Ref: [https://openaccess.thecvf.com/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf](https://openaccess.thecvf.com/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf)","metadata":{}},{"cell_type":"markdown","source":"## IV. Video Files Standardization ##\n\nPrepare video files into uniform format to be consumed by video classification.\n* Spatial dimension conversion:\n  * Resize avi files dimension from 320(width) x 240(height) to 128 (width)x171(height)\n  * Center crop fil to 112(width) x 112(height)\n  * ensure files loaded from OpenCV from BGR to RGB format\n* Temporal conversion:\n  * Clip video files into chunks of 32 frames\n* Serialize video chunks into TensorFlow TFRecord files.\n","metadata":{}},{"cell_type":"code","source":"# Common constants\nBASE_DIR = '/kaggle/input/ucf50-latest-version/UCF50/'\nTFRECORD_DIR = '/kaggle/working/ucf50-tfrec'\nSAMPLE_CATEGORIES = [\n    'HighJump', 'Basketball', 'TennisSwing', 'PullUps', 'PushUps',\n    'JumpingJack', 'SkateBoarding', 'TrampolineJumping', 'Lunges', 'GolfSwing'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T06:17:58.079683Z","iopub.execute_input":"2025-07-28T06:17:58.079978Z","iopub.status.idle":"2025-07-28T06:17:58.084147Z","shell.execute_reply.started":"2025-07-28T06:17:58.079955Z","shell.execute_reply":"2025-07-28T06:17:58.083481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### III.i Data Pre-processing ###\nimport os\nimport shutil\n\ndef clean_up_output_dir(): \n    if not os.path.exists(TFRECORD_DIR):\n        os.makedirs(TFRECORD_DIR, exist_ok=True)\n        print(f\"Output directory '{TFRECORD_DIR}' created.\")\n    else:\n        for subdir in os.listdir(TFRECORD_DIR):\n            full_path = os.path.join(TFRECORD_DIR, subdir)\n            if os.path.isdir(full_path):\n                shutil.rmtree(full_path) \n        print(f\"All files and subdirectory under '{TFRECORD_DIR}' removed.\")\n        os.makedirs(TFRECORD_DIR, exist_ok=True)\n        print(f\"Output directory '{TFRECORD_DIR}' re-created.\")\n    print('Output Clean-up complete')\n#clean_up_output_dir()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T06:18:01.147895Z","iopub.execute_input":"2025-07-28T06:18:01.148581Z","iopub.status.idle":"2025-07-28T06:18:01.153539Z","shell.execute_reply.started":"2025-07-28T06:18:01.148558Z","shell.execute_reply":"2025-07-28T06:18:01.152751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\n\n# Constants\nRESIZE_DIM = (128, 171)   # Resize first to this size\nCROP_SIZE = 112           # Then center crop to this\nNUM_FRAMES = 32           # Temporal length\nSTRIDE = 16               # Used for chunking long videos\n\ndef standardize_video_with_spatial_conversion(video_path):\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frames = []\n\n    for _ in range(total_frames):\n        success, frame = cap.read()\n        if not success:\n            break\n        frame = cv2.resize(frame, RESIZE_DIM)\n\n        # Center crop manually (corrected redundancy)\n        h, w = frame.shape[:2]\n        start_h = (h - CROP_SIZE) // 2\n        start_w = (w - CROP_SIZE) // 2\n        frame = frame[start_h:start_h + CROP_SIZE, start_w:start_w + CROP_SIZE]\n\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n        frames.append(frame)\n\n    cap.release()\n    return frames  # list of (112x112x3) numpy arrays\n\ndef standardize_video_with_temporal_conversion(frames):\n    chunks = []\n    if not frames:\n        return [] \n    if len(frames) < NUM_FRAMES:\n        # Pad with last frame\n        padded = frames + [frames[-1]] * (NUM_FRAMES - len(frames))\n        chunks.append(np.array(padded, dtype=np.uint8))\n    else:\n        # Break into overlapping chunks\n        for i in range(0, len(frames) - NUM_FRAMES + 1, STRIDE):\n            chunk = frames[i:i + NUM_FRAMES]\n            chunks.append(np.array(chunk, dtype=np.uint8))\n    return chunks\n\nprint('Spatial and Temporal conversion method defined.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T06:18:05.152435Z","iopub.execute_input":"2025-07-28T06:18:05.152714Z","iopub.status.idle":"2025-07-28T06:18:05.160983Z","shell.execute_reply.started":"2025-07-28T06:18:05.152696Z","shell.execute_reply":"2025-07-28T06:18:05.160227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\n# Serialize a clip as a TFRecord Example\ndef serialize_example(clip, label):\n    feature = {\n        'video': tf.train.Feature(bytes_list=tf.train.BytesList(value=[clip.tobytes()])),\n        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n        'shape': tf.train.Feature(int64_list=tf.train.Int64List(value=clip.shape))\n    }\n    example = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example.SerializeToString()\n\ndef preprocess_videos():\n    label_map = {cls: idx for idx, cls in enumerate(SAMPLE_CATEGORIES)}\n    for category in SAMPLE_CATEGORIES:\n        category_path = os.path.join(BASE_DIR, category)\n        output_category_dir = os.path.join(TFRECORD_DIR, category)\n        os.makedirs(output_category_dir, exist_ok=True)\n    \n        count = 0\n        for filename in tqdm(os.listdir(category_path), desc=f'Processing {category}'):\n            if not filename.endswith('.avi'):\n                continue\n    \n            video_path = os.path.join(category_path, filename)\n            converted_frames = standardize_video_with_spatial_conversion(video_path)\n            converted_chunks = standardize_video_with_temporal_conversion(converted_frames)\n    \n            for clip in converted_chunks:\n                tfrecord_filename = os.path.join(output_category_dir, f'{category}_{count:05d}.tfrecord')\n                with tf.io.TFRecordWriter(tfrecord_filename) as writer:\n                    example = serialize_example(clip, label_map[category])\n                    writer.write(example)\n                count += 1\n    print('video pre-processed complete.')\n\n# if os.path.exists(TFRECORD_DIR):\n#     clean_up_output_dir()\n# preprocess_videos()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T06:18:09.848188Z","iopub.execute_input":"2025-07-28T06:18:09.848877Z","iopub.status.idle":"2025-07-28T06:18:24.311814Z","shell.execute_reply.started":"2025-07-28T06:18:09.848852Z","shell.execute_reply":"2025-07-28T06:18:24.311030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport warnings\nimport tempfile\nimport tensorflow as tf\nimport numpy as np\nfrom IPython.display import Image, display\nfrom moviepy.editor import ImageSequenceClip\n\nwarnings.filterwarnings('ignore')\n\nif 'XDG_RUNTIME_DIR' not in os.environ:\n    os.environ['XDG_RUNTIME_DIR'] = '/tmp/runtime-dir'\n    os.makedirs(os.environ['XDG_RUNTIME_DIR'], exist_ok=True)\n\ndef parse_tfrecord(example_proto):\n    feature_description = {\n        'video': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.int64),\n        'shape': tf.io.FixedLenFeature([4], tf.int64),\n    }\n    parsed = tf.io.parse_single_example(example_proto, feature_description)\n    shape = parsed['shape']\n    video_raw = tf.io.decode_raw(parsed['video'], tf.uint8)\n    video = tf.reshape(video_raw, shape)\n    video = tf.cast(video, tf.float32) / 255.0  # Normalize if needed\n    return video, parsed['label']\n\n\ndef video_array_to_gif(video_array):\n    try:\n        with tempfile.NamedTemporaryFile(suffix='.gif', delete=False) as tmp_file:\n            gif_filename = tmp_file.name\n        resized_frames = [cv2.resize((frame * 255).astype(np.uint8), (200, 200)) for frame in video_array]\n        clip = ImageSequenceClip(resized_frames, fps=10)\n        clip.write_gif(gif_filename, program='ffmpeg', logger=None)\n        clip.close()\n        return gif_filename\n    except Exception as e:\n        print(f\"GIF creation error: {e}\")\n        return None\n\n\ntotal_tfref_count = 0\nfor category in SAMPLE_CATEGORIES:\n    category_path = os.path.join(TFRECORD_DIR, category)\n    if os.path.exists(category_path):\n        tfrec_files = [f for f in os.listdir(category_path) if f.endswith('.tfrecord')]\n        count = len(tfrec_files)\n        total_tfref_count += count\n        print(f\"Category: {category}, TFREC File Count: {count}\")\n\n        if count > 0:\n            sample_file = os.path.join(category_path, tfrec_files[0])\n            raw_dataset = tf.data.TFRecordDataset(sample_file).map(parse_tfrecord)\n\n            # Get first parsed example\n            for video_tensor, _ in raw_dataset.take(1):\n                video_np = video_tensor.numpy()  # Convert to NumPy\n                gif_file = video_array_to_gif(video_np)\n                if gif_file and os.path.exists(gif_file):\n                    display(Image(filename=gif_file))\n                    os.remove(gif_file)\n\nprint(f\"Total tfref files: {total_tfref_count}.\")\nprint('Display sample tfref files complete.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T06:26:23.760749Z","iopub.execute_input":"2025-07-28T06:26:23.761417Z","iopub.status.idle":"2025-07-28T06:26:24.878306Z","shell.execute_reply.started":"2025-07-28T06:26:23.761394Z","shell.execute_reply":"2025-07-28T06:26:24.877549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## V. Model Architecture ##\n\n### Model Description ###\n\nThis model implements 3D Conventional Neural Network for video classification.\n- **Input shape**: (batch_size=8, 32, 112, 112, 3)\n    - temporal depth 32\n    - spatial 112×112 RGB\n- **Layers**:\n    - Conv3D -> MaxPool3D -> Dropout (×3)\n    - Flatten -> Dense(256) -> Dropout -> Dense(10 softmax)\n- **Output**:\n    - 10 categories\n- **(Dropout)**:\n    - training=True/False based on call()\n- **Batch Size**\n    - default batch size is set to 8, which should fit the memory boundary of GPU P100\n\n### Key Design ###\n-  Model Init method accept two set of hyper-parameters\n    - **filters**: Number of filter in the Convolutional 3D layers\n    - **droputs**: Variable Dropout rates \n- Unlike 2D CNN network, 3D Convolutional Neural Network use a cube filter to capture both spatial (Height x Width x Channel) and temporal dimension (**Time**).\n- This model accept tensor input of (batch_size x height x width x channel x frames)\n- Base Model begins with flat 32 filters on all 3 stacks of Conv3D with dropout rate=0.5\n- During hyper-parameter tuning, we will experiment with ascending value of filter [32, 64, 128]\n- Then, we will experiment with ascending dropout rate [0.3, 0.4, 0.5]\n\n#### Reference ###\n- Ref: [https://www.tensorflow.org/tutorials/video/video_classification](https://www.tensorflow.org/tutorials/video/video_classification)\n\n<img src=\"https://www.tensorflow.org/images/tutorials/video/3DCNN.png\" width=\"600\">","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T06:20:48.717112Z","iopub.execute_input":"2025-07-28T06:20:48.718037Z","iopub.status.idle":"2025-07-28T06:20:48.721967Z","shell.execute_reply.started":"2025-07-28T06:20:48.718012Z","shell.execute_reply":"2025-07-28T06:20:48.721341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# During data-preprocessing step, we have limited dataset to 10 categories.\nOUTPUT_CLASS = 10\n\n# Custom Conv3D Model Class\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nOUTPUT_CLASS = 10\n\nclass VideoActionClassifier(tf.keras.Model):\n    \"\"\"\n    A flexible 3-layer Conv3D-based video classifier.\n    Accepts variable filters and dropout rates for hyper-parameter tuning.\n    Input shape: (batch, frames, height, width, channels)\n    \"\"\"    \n    def __init__(self, filters, dropouts):\n        super(VideoActionClassifier, self).__init__()\n\n        self.model_layers = [\n            layers.Conv3D(filters[0], kernel_size=(3, 3, 3), activation='relu', padding='same'),\n            layers.MaxPool3D(pool_size=(1, 2, 2), strides=(1, 2, 2)),\n            layers.Dropout(dropouts[0]),\n\n            layers.Conv3D(filters[1], kernel_size=(3, 3, 3), activation='relu', padding='same'),\n            layers.MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2)),\n            layers.Dropout(dropouts[1]),\n\n            layers.Conv3D(filters[2], kernel_size=(3, 3, 3), activation='relu', padding='same'),\n            layers.MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2)),\n            layers.Dropout(dropouts[2]),\n\n            layers.Flatten(),\n            layers.Dense(256, activation='relu'),\n            layers.Dropout(0.5),\n            layers.Dense(OUTPUT_CLASS, activation='softmax')\n        ]\n\n    def call(self, x, training=False):\n        for layer in self.model_layers:\n            if isinstance(layer, layers.Dropout):\n                x = layer(x, training=training)\n            else:\n                x = layer(x)\n        return x\n\n# hyper-parameters\ndropout_param1 = [0.3, 0.3, 0.3]\nfilter_param1 = [32, 32, 32]\ndropout_param2 = [0.3, 0.4, 0.5]\nfilter_param2 = [32, 64, 128]\n\n# batch_size=8 is safe for 32x112x112x3x8 ~ 160MB RAM\n# should fit GPU P100\nBATCH_SIZE = 8\n\n# Base model\nbase_model = VideoActionClassifier(filters=filter_param1, dropouts=dropout_param1)\nbase_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n#input shape 112(h) x 112(w) x3(channel) x 32 (frame)\nbase_model.build(input_shape=(None, 32, 112, 112, 3))\nbase_model.save('conv3d_base_model.h5')\nbase_model.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T06:20:52.052943Z","iopub.execute_input":"2025-07-28T06:20:52.053529Z","iopub.status.idle":"2025-07-28T06:20:52.185782Z","shell.execute_reply.started":"2025-07-28T06:20:52.053509Z","shell.execute_reply":"2025-07-28T06:20:52.185219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split tfref fileset into training, validate, and test dataset using stratify splitting.\n# Stratify splitting will ensure each category of video class be split proportionally.\nimport os\nimport random\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport glob\n\nTFRECORD_DIR = '/kaggle/working/ucf50-tfrec'\nSPLIT_RATIO = [0.7, 0.15, 0.15]\nBATCH_SIZE=8\n\ndef get_paths_and_labels():\n    paths = []\n    labels = []\n    for idx, category in enumerate(sorted(os.listdir(TFRECORD_DIR))):\n        category_path = os.path.join(TFRECORD_DIR, category)\n        if not os.path.isdir(category_path):\n            continue\n        files = glob.glob(os.path.join(category_path, '*.tfrecord'))\n        paths.extend(files)\n        labels.extend([idx] * len(files))\n    return paths, labels\n\ndef stratified_split():\n    paths, labels = get_paths_and_labels()\n    train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n        paths, labels, test_size=SPLIT_RATIO[2], random_state=SEED, stratify=labels)\n\n    train_paths, val_paths, _, _ = train_test_split(\n        train_val_paths, train_val_labels,\n        test_size=SPLIT_RATIO[1] / (SPLIT_RATIO[0] + SPLIT_RATIO[1]),\n        random_state=SEED,\n        stratify=train_val_labels)\n\n    return train_paths, val_paths, test_paths\n\ndef load_dataset(paths, batch_size=BATCH_SIZE):\n    dataset = tf.data.TFRecordDataset(paths)\n    dataset = dataset.map(_parse_example, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.cache().shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return dataset\n\ntrain_paths, validate_paths, test_paths = stratified_split()\ntrain_ds = load_dataset(train_paths)\nvalidate_ds = load_dataset(validate_paths)\ntest_ds = load_dataset(test_paths)\nprint(f\"Train: {len(train_paths)} TFRecords\")\nprint(f\"Validation: {len(validate_paths)} TFRecords\")\nprint(f\"Test: {len(test_paths)} TFRecords\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import callbacks\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nPATIENCE_VALUE=5\n\nearly_stop = callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE_VALUE, restore_best_weights=True)\ncheckpoint = callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_accuracy')\n\nhistory = base_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=30,\n    callbacks=[early_stop, checkpoint]\n)\ntest_loss, test_accuracy = base_model.evaluate(test_ds)\nprint(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n\n# display confusion matrix\ny_true_base_model = []\ny_pred_base_model = []\n\nfor videos, labels in test_ds:\n    preds_base_model = base_model.predict(videos)\n    y_true_base_model.extend(labels.numpy())\n    y_pred_base_model.extend(np.argmax(preds_base_model, axis=1))\n\ncm_base_model = confusion_matrix(y_true_base_model, y_pred_base_model)\n# disp = ConfusionMatrixDisplay(confusion_matrix=cm_base_model)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_base_model, display_labels=SAMPLE_CATEGORIES)\nplt.figure(figsize=(10, 8))\ndisp.plot(cmap='Blues', values_format='d')\nplt.title(\"Confusion Matrix on Test Set\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.grid(False)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot Loss\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Train Loss', marker='o')\nplt.plot(history.history['val_loss'], label='Val Loss', marker='o')\nplt.title('Training vs Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\n# Plot Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Train Acc', marker='o')\nplt.plot(history.history['val_accuracy'], label='Val Acc', marker='o')\nplt.title('Training vs Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# # Tuned filters only\n# model_filter_tuning = VideoActionClassifier(filters=filter_param2, dropouts=dropout_param1)\n# model_filter_tuning.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n#                loss='sparse_categorical_crossentropy',\n#                metrics=['accuracy'])\n\n# # Tuned filters and dropouts\n# model_filter_dropout_tuning = VideoActionClassifier(filters=filter_param2, dropouts=dropout_param2)\n# model_filter_dropout_tuning.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n#                loss='sparse_categorical_crossentropy',\n#                metrics=['accuracy'])\n\n# # parameter tuning model will be built at later stage.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## VI. Result Analysis Before Fine-Tuning ##\n\n- Overall, this CNN model performs moderately for the histopathologic cancer detection task before tuning, as it has a simple architecture and achieves 87% accuracy on the validation dataset. \n\n### Details ###\n- At the end, this simplistic CNN model achieves 94% accuracy in the training dataset, but drops accuracy to 87% in the validation set. The drop in accuracy suggested that the model might have overfit the training dataset.\n- The curve of training loss starts at 0.4 and decreases steadily to 0.1 around epoch 8. However, validation loss spikes sharply, and this suggest the model is memorizing the training data rather than generalizing it.\n- The accuracy curve follows similiar patterns as in the curve of the training loss. Even though there are spikes with the validation curve, the validation accuracy improved gradually.\n- Spikes are also observed in the AUC curve, but it did achieve the highest value of 0.97, suggesting the model is highly effective despite fluctuations.\n","metadata":{}},{"cell_type":"markdown","source":"## VI. Explore Different CNN Model Architectures ##\n\nMy approach to explore different Convolutional Neural Network (CNN) architectures is as follows:\n\n- Compare CNN model performance of same layout with higher and lower convolution layers\n  - **2 convolutional layers**\n  - **3 convolutional layers**\n  - **5 convolutional layers**\n","metadata":{}},{"cell_type":"code","source":"print('V')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## VII. Performance Analysis with Different Model Architectures ##\n\n- Deeper model achieve higher traing and validation accuracy\n  - 5 layer CNN reach 97% training accuracy, compared to 94% (3 layers), and 84% (2 layers)\n  - 5 layer CNN reach 93% training accuracy, compared to 88% (3 layers), and 86% (2 layers)\n  - Deeper network have greater capacity to learn more complex pattern\n\n- Deeper model may introduce higher risk of overfitting\n  - 5 layer model have relatively deeper gap between training and validation accuracy.\n  - Performance gap widens with depth, suggesting a higher risk of overfitting and early stopping is required (next performance tuning).","metadata":{}},{"cell_type":"code","source":"print('VI')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## VIII. Hyper-parameter Fine-Tuning by reducing Number of Filters ##\n\n* **Validation accuracy lower than training accuracy** indicates **overfitting**, where the model performs well on training data but fails to generalize to unseen data.\n* Overfitting may result from too many filters that learn noise patterns from the training data.\n* Less number of filters encouraging to focus on essential patterns rather than memorizing training data.\n\n- Three set number of filters be used for performance comparison:\n  - (16, 32, 64, 128, 256), \n  - (24, 48, 96, 192, 384)\n  - (32, 64, 128, 256, 512) (best model architecture)\n","metadata":{}},{"cell_type":"markdown","source":"## XII. Conclusion ##\n\n### Best performance is achieved with the following configurations ###\n- Model Architecture: 5 convoluntion block + output block\n- Input shape: (96, 96, 3)\n- Kernel size=5 for the 1st convolution block, and kernel size=3 for the rest.\n- Stride=1, padding=same,\n- Each convolution block include batchnormation max pooling size=2, strides=2\n- Filter configuration: 32, 64, 128, 256, 512\n- Learning rate: default (0.001) with Early stopping\n\n\nDiscuss and interpret results as well as learnings and takeaways. What did and did not help improve the performance of your models? What improvements could you try in the future?","metadata":{}},{"cell_type":"markdown","source":"### Results ###\n- High precision and specificity mean the model is reliable with high numbers of TP and TN\n- Moderate false negative rate (1,881 missed cases) \n\n\n| Metric               | Formula                          | Value     |\n|----------------------|----------------------------------|-----------|\n| Accuracy             | (TP + TN) / (TP + TN + FP + FN) | 0.9348    |\n| Precision (PPV)      | TP / (TP + FP)                  | 0.9489    |\n| Recall (Sensitivity) | TP / (TP + FN)                  | 0.8946    |\n| Specificity          | TN / (TN + FP)                  | 0.9672    |\n| F1 Score             | 2 * (Precision * Recall) / (Precision + Recall) | 0.9209    |\n\nConfusion Matrix:\n|                  | Predicted Negative | Predicted Positive | Total   |\n|------------------|--------------------|---------------------|---------|\n| **Actual Negative** | TN = 25,322 (58.9%) | FP = 860 (2.0%)       | 26,182 (60.9%) |\n| **Actual Positive** | FN = 1,881 (4.4%)   | TP = 15,942 (37.1%)    | 17,823 (39.1%) |\n| **Total**          | 27,203 (63.2%)     | 16,802 (36.8%)         | 43,005 (100%) |\n","metadata":{}},{"cell_type":"markdown","source":"### What improve the performance of the model ###\n- Deeper model achieve higher traing and validation accuracy\n  - 5 layer CNN reach 97% training accuracy, compared to 94% (3 layers), and 84% (2 layers)\n  - 5 layer CNN reach 93% training accuracy, compared to 88% (3 layers), and 86% (2 layers)\n  - Deeper network have greater capacity to learn more complex pattern\n\n- Deeper model may introduce higher risk of overfitting\n  - 5 layer model have relatively deeper gap between training and validation accuracy.\n  - Performance gap widens with depth, suggesting a higher risk of overfitting and early stopping is required.\n\n- Increase filter depth helped feature extraction:\n  - The spike in the validation history suggests that the model has learned enough useful features to achieve a high AUC score, but still requires further regularization.\n  - Even though fewer filter configurations (24, 48, 96, 192, 384) might have improved regularization, they do not learn enough complex features to exceed the original (32, 64, 128, 256, 512) filter configuration.  -     - Filter configuration (16, 32, 64, 128, 256) performs the worst because it lacks the capacity to capture complex patterns.\n  - Thus, for the best model submission, **filter configurations (32, 64, 128, 256, 512) remains the best choice.**\n\n- Early stopping protects against overfitting but requires appropriate learning rates for optimal effect.\n  - Learning rate =0.001 (default) achieve the highest accuracy (0.93) and AUC (0.97)\n  - When the learning rate is reduced (rate = 0.0005), slower convergence might have caused the model to underfit before early stopping, halting the training.\n  - When the learning rate is increased (rate=0.005), faster convergence may cause instability without reaching the optimum.\n\n### What improvement could you try in the future? ###\n- More complex convolution model such as VGG-16/ VGG-32 by Oxford University [https://www.robots.ox.ac.uk/~vgg/research/very_deep/](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n- Explore vision transformer(ViT) developed by Google Deep Brain Team [https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/](https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/)","metadata":{}},{"cell_type":"markdown","source":"## XIII. Future Works and References ##\n- Experiment with VGG-16, VGG-32 by Oxford University [https://www.robots.ox.ac.uk/~vgg/research/very_deep/](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n- Explore with Visual Transformer by Google Deep Brain [https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/](https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/)\n\n### References ###\n- Ref: Tensor flow tutorial [https://www.tensorflow.org/tutorials](https://www.tensorflow.org/tutorials) \n- Ref: TensorFlow Guide: [https://www.tensorflow.org/guide/keras/](https://www.tensorflow.org/guide/keras/)","metadata":{}}]}